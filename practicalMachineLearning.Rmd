---
title: "Prediction of Weight-Lifting Exercise Form from Inertial Measurement Unit Data"
author: "Chris W"
date: "November 22, 2015"
output:
  html_document:
    fig_caption: yes
    theme: readable
  pdf_document:
    fig_caption: yes
  word_document:
    fig_caption: yes
---

# Data Analysis

## Summary

The following is a description of the analysis performed for the course project of "Practical Machine Learning" from Johns Hopkins University/Coursera. The project is based on a Human Activity Recognition dataset from  http://groupware.les.inf.puc-rio.br/har, the goal being to classify correct or incorrect forms of a weight lifting exercise based on collected accelerometer data.

## Experimental Data

The experimental data is based on the Weight Lifting Exercises Dataset from http://groupware.les.inf.puc-rio.br/har.  In this dataset, six participants performed one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl under different specifications or forms.  One of these specifications was the correct form, while four other specifications represented incorrect forms or mistakes commonly made.

Motion data was tracked with four Razor inertial meaurements units (IMU) attached to the participants' glove, armband, lumbar belt and dumbbell.  Each IMU provides a time series of three-axeis acceleration, gyroscope and magnetometer data obervations with multiple samples per second.

The experiment is more fully described in references 1) and 2).

## Exploratory Data Analysis

The data was separated into training and test sets available from:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv .

```{r libraries, echo=TRUE, message=FALSE}
# Load packages
library(knitr)
library(dplyr)
library(reshape2)
library(ggplot2)
library(caret)
library(doParallel)
```

```{r loadHelpers, ref.label='helpers', echo=FALSE}
```


```{r, loadRawTrain, echo=TRUE, cache=TRUE}
rawTrain <- read.table('pml-training.csv', header = TRUE, sep = ',', na.strings=c("NA", "#DIV/0!"), stringsAsFactors = FALSE)
```

The training data consists of `r nrow(rawTrain)` observations of `r ncol(rawTrain)` variables. Examination of the data shows that the bulk of the observations consists of times series data with a minority of rows being summary calculations at the end of one second boundaries. The summary data accounts for over half of the variables.  Since the test (prediction) data consists of only instantaneous observations, the summary observations and association variables are removed.  

Several columns were found to be missing for one or more users, for one or more activities.  These columns were also dropped form the dataset rather than chosing to impute values.  A very small number of outliers were adjusted by hand.

```{r, dropTrainSummaryData, echo=TRUE, cache=FALSE}
  # see appendix for helper functions 

  # remove the 'summary' lines that appear when new_window is 'yes'
  train <- dropSummaryData(rawTrain)
  train <- dropMissingColumns(train)
  # hand correct small number of outliers
  train <- fixMissingValues( train ) 
  
  # Combine the timestamps.  This will be used only for exploratory purposes
  # but will not be used in the main modelling
  train$timestamp <- train$raw_timestamp_part_1 + train$raw_timestamp_part_2 / 1000000
```  

This reduces the training dataset to `r nrow(train)` observations and `r ncol(train)` variables.  The remaining variables are shown below.

```{r, trainVariables, echo=FALSE, cache=FALSE}
  colnames(train)
```  

### Activity Duration

```{r, calcDuration, echo=TRUE, cache=FALSE}
durations <- train %>% group_by(user_name, classe) %>%
  summarize(min=min(timestamp), 
            max = max(timestamp),
            range = max-min,
            duration = round(range,1),
            numSamples = n(),
            avgSamplesPerSec = round(n()/range,1)) 
avgDuration <- mean(durations$duration)
avgSamplesPerSec <- mean(durations$avgSamplesPerSec)

```

We find that on average each activity takes `r round(avgDuration,1)` seconds, and that there are `r round(avgSamplesPerSec,1)` samples/sec.  This sampling rate is approximately one half that of the IMU device rating rate specified in reference 2).  It appears that the dataset has been reduced down from the original experimental data and this can be confirmed by comparing against the original dataset (reference 3).  

```{r, durationTable, echo=FALSE, cache=FALSE}
kable(dcast(melt(durations, id.vars=c("user_name","classe"), 
                 measure.vars = "duration"), classe~user_name),
      caption = "Duration of activity per participant (seconds)")

```

### Examination of Sample Data

Viewing one of the variables (magnet_dumbbell_z) for one user there is indication that the pattern of observations does vary for different forms of the exercise. Similarly a density plot for the pitch_belt variables for the user charles also shows separation between the different forms of exercise.

```{r, sampleVariable, echo=FALSE, fig.cap = "Sample data: magnet_dumbbell_z for one participant (adelmo) across all exercise forms (classe)"}
g <- ggplot(train[train$user_name=='adelmo',],aes(x=timestamp, y=magnet_dumbbell_z)) +
  geom_point() + facet_wrap(~classe, scales="free_x") +
  ggtitle("Variation of a sample variable over time for different activites") +
  theme(axis.text.x = element_blank())
g
```

```{r, sampleDensity, echo=FALSE, fig.cap = "Sample data: density plot of pitch_belt for one participant (charles) showing separation between different exercise forms (classe)"}
  g <- ggplot(train[train$user_name=='charles',], 
              aes(x=pitch_belt, col=classe))+ 
    geom_density() +
    ggtitle("Density of a sample variable for different activites")
  print(g)
```


## Modelling

### Algorithm Selection
In general the patterns of observations are very variable, and also cyclical over time. For this reason it appears that a decision tree analysis may be more appropriate than a linear or logistic regression approach.  Random Forest was chosen as the algorithm to fit the data.

### Variable Selection
Time-based variables, and user_name variables were not included in the model.  While these fields likely *do* have predictive power (see Appendix), the decision was made to fit a model that was independent the user -- ideally such a model would allow detection of exercise form for new users without retraining.

### Splitting of Datasets
In order to later compare CV-estimated prediction error with actual out of sample error, the training data is split into a training and validation dataset.

```{r, splitData, echo=TRUE}
set.seed(4858)
idx <- createDataPartition(train$classe, p=.8, list=FALSE)
reducedTrain <- train[idx,]
validation <- train[-idx,]
# select only IMU data variables, droping time and user variables
reducedTrain <- reducedTrain[,grepl('acc|mag|gyr|roll|pitch|yaw|classe', colnames(train))]
```

### Model Training

The model was trained using the caret package using default random forest model parameters for the number of trees and number of variables sampled at each split.

```{r, fitModel, echo=TRUE, cache=TRUE}
# see appendix for helper functions 

# Train random forest model using the caret package
mtry = floor(sqrt(ncol(reducedTrain)-1))
modelFit <- mytrain.rf(x = drop_df_cols(reducedTrain,c("classe")), 
                       y =  reducedTrain$classe,
                       mtry=mtry)
```

```{r, inSampleError, echo=TRUE, cache=FALSE}
# Calculate in-sample error rates
conf <- modelFit$finalModel$confusion[,1:5]
inSampleErrorRate =  100* (sum(conf) - sum(diag(conf))) / sum(conf)
```

The confusion matrix for the in-sample predictions shows that the overall accuracy of the model against the training data is very high, with an error rate of `r round(inSampleErrorRate,2) `%.

```{r, fitModelConfusion, echo=TRUE, cache=FALSE}
modelFit$finalModel$confusion
```

Since the accuracy of the model is very high, no further tuning of model paramters is performed.

## Error Analysis

A comparison of out of sample predictions against actual classification in the validation set again shows high accuracy, suggesting that the model generalizes well.

```{r, oosError, echo=FALSE, message=FALSE}
oosConf <- confusionMatrix(predict(modelFit, newdata = validation), validation$classe)
oosConf
```

Additionally, cross validation estimate of prediction error is compared to the actual out of sample error rate using the validation set.  

If used appropriately, cross validation provides a reasonable estimate of expected error (reference 4 - Elements of Statistical Learning p257.) In the above analysis all predictor filtering and selection has been performed independently of the response variable (classe).  Such filtering can legitimately be applied prior to cross validation, and does not bias the prediction error (reference 4 p246-7).

We find that all measures of prediction error are in relatively close agreement, with the CV error estimate somewhat above the out of sample error rate as measured against the validation set.

```{r, errors, echo=TRUE}
validationErrorRate <- 100*(1-oosConf$overall[1])
cvErrorRate <- (1 - mean(modelFit$resample$Accuracy))*100
cvErrorStd <- 100 * modelFit$results$AccuracySD
```

```{r, errComparison, echo=FALSE}
kable(data.frame( "In Sample Error %" = round(inSampleErrorRate,2), 
            "CV Error (Est OOS) %" = round(cvErrorRate,2),
            "CV Error Std Dev" = round(cvErrorStd,2),
            "Validation Error (OOS) %" = round(validationErrorRate,2), check.names=F),
      caption="Comparison of in-sample, cv and validation error rates.")

```

## Prediction

The final step is to make predictions against the test data.  Results will not be shown here but the final model predicts all twenty test samples correctly.

```{r, predict, echo=TRUE}
test <- read.table('pml-testing.csv', header = TRUE, sep = ',', na.strings=c("NA", "#DIV/0!"), stringsAsFactors = FALSE)

#test <- loadData('pml-testing.csv')
test <- dropSummaryData(test)
test <- dropMissingColumns(test)
test <- test[,grepl('acc|mag|gyr|roll|pitch|yaw|classe', colnames(train))]
pred <- predict(modelFit,test)
```

## Appendix

### Variable Importance

For interest, the top twenty most important variables for building the model are shown below. Six variables are found to contribute most significantly.
```{r, variableImporance, echo=TRUE, fig.cap="Most important variables found by the random forest final model."}
  varImpPlot(modelFit$finalModel, n.var=20)
```

### Time-Based Model

The exercises performed by each subject were conducted in series with no overlap.  In principle then, one could predict the form of exercise (classe) based solely on the knowledge of the wall clock time of an observation.  Can we train such a model using random forest?

```{r, timeModel, echo=TRUE, cache=TRUE}
# Form an accurate timestamp combining the seconds and fractional seconds variables
timeTrain <- train[idx,]

# Form a training set consisting only of the timestamp the response variable
timeTrain <- timeTrain[,grepl('classe|^timestamp$', colnames(timeTrain))]

# fit classe against time
timeFit <- mytrain.rf(x = drop_df_cols(timeTrain,c("classe")), 
                       y =  timeTrain$classe,
                       mtry=c(1))

# compare against the validation set
timeValidation <- validation
timeValidation$timestamp <- timeValidation$raw_timestamp_part_1 + timeValidation$raw_timestamp_part_2 / 1000000
confusionMatrix(predict(timeFit, newdata = timeValidation), timeValidation$classe)
```

It is found that using time alone produces a highly accurate model -- more accurate that using all of the actual instrument data! (Interestingly, there are still some errors.  On inspection of a sample of these, the errors were found to occur at the boundaries between participants.) However, the time based model would be useless to predict any observations taken outside the training data timeframes. 

If one wants to form a scientifically useful model, then the model should be based on variables that have high predictive value but that generalize well.  It is interesting to note that the observed dependence on time suggests that further review of the predictors could be warranted.  For example, should any of the instruments display a significant drift over time, if the data is used without correction then the drift alone could be a false predictor of activity type.

### Helper Functions

Helper functions used in the analysis are shown below.

```{r helpers, echo=TRUE, message=FALSE}

# hard coded list of summary variables
discardCols <- c(
  "kurtosis_roll_belt",      "kurtosis_picth_belt",     "kurtosis_yaw_belt",      
  "skewness_roll_belt",      "skewness_roll_belt.1",    "skewness_yaw_belt",      
  "max_roll_belt",           "max_picth_belt",          "max_yaw_belt",           
  "min_roll_belt",           "min_pitch_belt",          "min_yaw_belt",           
  "amplitude_roll_belt",     "amplitude_pitch_belt",    "amplitude_yaw_belt",     
  "var_total_accel_belt",    "avg_roll_belt",           "stddev_roll_belt",       
  "var_roll_belt",           "avg_pitch_belt",          "stddev_pitch_belt",      
  "var_pitch_belt",          "avg_yaw_belt",            "stddev_yaw_belt",        
  "var_yaw_belt",            "var_accel_arm",           "avg_roll_arm",           
  "stddev_roll_arm",         "var_roll_arm",            "avg_pitch_arm",          
  "stddev_pitch_arm",        "var_pitch_arm",           "avg_yaw_arm",            
  "stddev_yaw_arm",          "var_yaw_arm",             "kurtosis_roll_arm",      
  "kurtosis_picth_arm",      "kurtosis_yaw_arm",        "skewness_roll_arm",      
  "skewness_pitch_arm",      "skewness_yaw_arm",        "max_roll_arm",           
  "max_picth_arm",           "max_yaw_arm",             "min_roll_arm",           
  "min_pitch_arm",           "min_yaw_arm",             "amplitude_roll_arm",     
  "amplitude_pitch_arm",     "amplitude_yaw_arm",       "kurtosis_roll_dumbbell", 
  "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell",   "skewness_roll_dumbbell", 
  "skewness_pitch_dumbbell", "skewness_yaw_dumbbell",   "max_roll_dumbbell",      
  "max_picth_dumbbell",      "max_yaw_dumbbell",        "min_roll_dumbbell",      
  "min_pitch_dumbbell",      "min_yaw_dumbbell",        "amplitude_roll_dumbbell",
  "amplitude_pitch_dumbbell","amplitude_yaw_dumbbell",  "var_accel_dumbbell",     
  "avg_roll_dumbbell",       "stddev_roll_dumbbell",    "var_roll_dumbbell",      
  "avg_pitch_dumbbell",      "stddev_pitch_dumbbell",   "var_pitch_dumbbell",     
  "avg_yaw_dumbbell",        "stddev_yaw_dumbbell",     "var_yaw_dumbbell",       
  "kurtosis_roll_forearm",   "kurtosis_picth_forearm",  "kurtosis_yaw_forearm",   
  "skewness_roll_forearm",   "skewness_pitch_forearm",  "skewness_yaw_forearm",   
  "max_roll_forearm",        "max_picth_forearm",       "max_yaw_forearm",        
  "min_roll_forearm",        "min_pitch_forearm",       "min_yaw_forearm",        
  "amplitude_roll_forearm",  "amplitude_pitch_forearm", "amplitude_yaw_forearm",  
  "var_accel_forearm",       "avg_roll_forearm",        "stddev_roll_forearm",    
  "var_roll_forearm",        "avg_pitch_forearm",       "stddev_pitch_forearm",   
  "var_pitch_forearm",       "avg_yaw_forearm",         "stddev_yaw_forearm",     
  "var_yaw_forearm" 
)


# drop columns that are not going to be used for modelling
dropSummaryData <- function(data) {
  # remove the 'summary' lines that appear when new_window is 'yes'
  idx <- data$new_window != "yes"
  data <- data[idx,]
  
  # drop 'summary' columns
  data <- data[ , !names(data) %in% discardCols]
  
  return(data)
}

dropMissingColumns <- function(data) {
  
  # drop columns that are all zeros for one or more users
  # (may return to impute later)
  data <- subset(data, select = -c(roll_forearm, pitch_forearm, yaw_forearm,
                                   roll_arm, pitch_arm, yaw_arm))
  return(data)
}

fixMissingValues <- function(data) {
  # single huge outlier after trimming start of correct exercises
  # fix by hand
  data$magnet_dumbbell_y[data$X==9274]  <- 578
  
  return(data)
}
# delete a set of columns from a dataset
# http://stackoverflow.com/questions/5234117/how-to-drop-columns-by-name-in-a-data-frame
drop_df_cols <- function(df, names) {
  df[names] <- list(NULL)
  df
}


# http://jaehyeon-kim.github.io/r/2015/05/30/Setup-Random-Seeds-on-Caret-Package/
# function to set up random seeds for caret
setSeeds <- function(method = "cv", numbers = 1, repeats = 1, tunes = NULL, seed = 1237) {
  #B is the number of resamples and integer vector of M (numbers + tune length if any)
  B <- if (method == "cv") numbers
  else if(method == "repeatedcv") numbers * repeats
  else NULL
  
  if(is.null(length)) {
    seeds <- NULL
  } else {
    set.seed(seed = seed)
    seeds <- vector(mode = "list", length = B)
    seeds <- lapply(seeds, function(x) sample.int(n = 1000000, size = numbers + ifelse(is.null(tunes), 0, tunes)))
    seeds[[length(seeds) + 1]] <- sample.int(n = 1000000, size = 1)
  }
  seeds
}


# function to set up a cluster and train random forest model using caret
# x data set as matrix
# y vector of predictors
# mtry vector of values to tune over
mytrain.rf <- function( x, y, ntree=500, mtry)
{
  numFolds <- 10
  
  tuneGrid <- expand.grid(mtry = mtry)
  cvSeeds <- setSeeds(method = "cv", numbers = numFolds, tunes = length(tuneGrid), 
                      seed = 4858)  
  trc <- trainControl(number = numFolds, method = "cv", savePredictions = "all",
                      seeds = cvSeeds)

  cl = makeCluster(3)
  registerDoParallel(cl)
  set.seed(4858)
  modelFit <- train(x = x, y =  y, method="rf", trControl = trc, 
                    ntree=ntree, tuneGrid = tuneGrid)
  stopCluster(cl)
  return(modelFit)
}



```


## References
1) http://groupware.les.inf.puc-rio.br/har#ixzz3qjdzbokg
2) "Qualitative Activity Recognition of Weight Lifting Exercises" http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201
3) Full WLE dataset http://groupware.les.inf.puc-rio.br/static/WLE/WearableComputing_weight_lifting_exercises_biceps_curl_variations.csv
4) Elements of Statistical Learning, Second edition.  Hastie, Tibshirani & Friedman


```{r, echo=FALSE, cache=FALSE}
# Helpful tip from Keith Derrick -- save off environment
save.image(file="markdown_results.rdata")
```
